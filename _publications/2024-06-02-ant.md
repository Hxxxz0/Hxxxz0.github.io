---
title: "ANT: Adaptive Neural Temporal-Aware Text-to-Motion Model"
collection: publications
category: manuscripts
permalink: /publication/2024-06-02-ant
excerpt: 'An adaptive neural temporal-aware model for text-to-motion generation with improved semantic alignment and generation efficiency.'
date: 2024-06-02
venue: 'arXiv preprint'
slidesurl: # 'http://academicpages.github.io/files/slides4.pdf'
paperurl: 'https://arxiv.org/abs/2506.02452'
citation: 'Jia, H. et al. (2024). "ANT: Adaptive Neural Temporal-Aware Text-to-Motion Model." <i>arXiv preprint arXiv:2506.02452</i>.'
---

## Abstract

This paper proposes ANT (Adaptive Neural Temporal-Aware Text-to-Motion Model), which significantly improves semantic alignment and generation efficiency in text-to-motion tasks. By introducing semantic temporal-aware modules and Dynamic Classifier-Free Guidance (DCFG) strategy, our model better understands and generates temporally and semantically coherent motions.

## Key Contributions

- **Temporal Awareness**: Novel semantic temporal-aware module design
- **Dynamic Guidance**: DCFG strategy for improved generation control
- **Enhanced Alignment**: Better semantic alignment between text and motion
- **Improved Efficiency**: Faster generation with maintained quality

## Technical Highlights

- Temporal-aware neural architecture
- Dynamic classifier-free guidance mechanism
- Improved semantic understanding for motion generation
- Efficient training and inference pipeline

[Download paper here](https://arxiv.org/abs/2506.02452) 